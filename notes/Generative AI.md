# Generative AI

### L'intelligenza artificiale generativa sta cambiando tutto. Ma cosa rimane quando l'hype è finito?

Era chiaro che OpenAI aveva in mente qualcosa. Alla fine del 2021, un piccolo gruppo di ricercatori stava giocando con una nuova versione del modello text-to-image di OpenAI, DALL-E, un'intelligenza artificiale che converte brevi descrizioni scritte in immagini: una volpe dipinta da Van Gogh, forse, o un corgi fatto di pizza. Ora si trattava solo di capire cosa farne.

Nessuno poteva prevedere quanto sarebbe stato grande il successo di questo prodotto. Il rapido rilascio di altri modelli generativi ha ispirato centinaia di titoli di giornali e copertine di riviste, ha riempito i social media di meme, ha messo in moto la macchina dell'hype e ha scatenato un'intensa reazione da parte dei creatori.

> L'eccitante verità è che non sappiamo cosa ci aspetta. Anche se le industrie creative sentiranno per prime l'impatto, questa tecnologia darà superpoteri creativi a tutti. A lungo termine, potrebbe essere utilizzata per generare progetti per quasi tutto. Il problema è che questi modelli non hanno ancora idea di cosa stiano facendo.
*—Will Douglas Heaven*

Era chiaro che OpenAI aveva in mente qualcosa. Alla fine del 2021, un piccolo team di ricercatori stava giocando con un'idea nella sede di San Francisco dell'azienda. Avevano costruito una nuova versione del modello text-to-image di OpenAI, DALL-E, un'intelligenza artificiale che converte brevi descrizioni scritte in immagini: una volpe dipinta da Van Gogh, forse, o un corgi fatto di pizza. Ora si trattava solo di capire cosa farne.

"Quasi sempre costruiamo qualcosa e poi tutti dobbiamo usarlo per un po'", racconta Sam Altman, cofondatore e CEO di OpenAI, al MIT Technology Review. "Cerchiamo di capire cosa sarà, a cosa servirà".

Non questa volta. Man mano che lavoravano con il modello, tutti i partecipanti si sono resi conto che si trattava di qualcosa di speciale. "Era molto chiaro che si trattava di questo, del prodotto giusto", dice Altman. "Non c'è stato alcun dibattito. Non abbiamo mai fatto una riunione al riguardo".

Ma nessuno, né Altman né il team di DALL-E, poteva prevedere quanto sarebbe stato grande il successo di questo prodotto. "Questa è la prima tecnologia AI che ha preso piede tra la gente comune", dice Altman.  

DALL-E 2 è uscito nell'aprile 2022. A maggio, Google ha annunciato (ma non ha rilasciato) due propri modelli di text-to-image, Imagen e Parti. Poi è arrivato Midjourney, un modello text-to-image realizzato per gli artisti. Ad agosto è arrivato Stable Diffusion, un modello open-source che la startup britannica Stability AI ha rilasciato gratuitamente al pubblico.

Le porte erano fuori dai cardini. OpenAI ha registrato un milione di utenti in soli 2,5 mesi. In meno della metà del tempo, più di un milione di persone ha iniziato a utilizzare Stable Diffusion tramite il servizio a pagamento Dream Studio; molte altre hanno utilizzato Stable Diffusion tramite applicazioni di terze parti o hanno installato la versione gratuita sui propri computer. (Emad Mostaque, fondatore di Stability AI, dice di puntare a un miliardo di utenti).

Poi, a ottobre, c'è stato il secondo round: una serie di modelli text-to-video di Google, Meta e altri. Invece di generare solo immagini fisse, questi modelli sono in grado di creare brevi videoclip, animazioni e immagini 3D.  

Il ritmo di sviluppo è stato mozzafiato. In pochi mesi, la tecnologia ha ispirato centinaia di titoli di giornali e copertine di riviste, ha riempito i social media di meme, ha messo in moto la macchina dell'hype e ha scatenato un'intensa reazione.

"Lo shock e lo stupore di questa tecnologia sono sorprendenti - ed è divertente, è ciò che la nuova tecnologia dovrebbe essere", afferma Mike Cook, ricercatore di IA al King's College di Londra che studia la creatività computazionale. "Ma la velocità è tale che le impressioni iniziali vengono aggiornate prima ancora che ci si abitui all'idea. Credo che la società ci metterà un po' a digerirla".

Gli artisti si trovano nel mezzo di uno dei più grandi sconvolgimenti di una generazione. Alcuni perderanno il lavoro, altri troveranno nuove opportunità. Alcuni si stanno rivolgendo ai tribunali per combattere battaglie legali contro quella che considerano un'appropriazione indebita di immagini per formare modelli che potrebbero sostituirli.

I creatori sono stati colti alla sprovvista, afferma Don Allen Stevenson III, un artista digitale californiano che ha lavorato presso studi di effetti visivi come la DreamWorks. "Per persone tecnicamente preparate come me, è molto spaventoso. Ti viene da pensare: 'Oh mio Dio, questo è il mio lavoro'", dice. "Sono entrato in crisi esistenziale per il primo mese di utilizzo di DALL-E".

Ma mentre alcuni si stanno ancora riprendendo dallo shock, molti - tra cui Stevenson - stanno trovando il modo di lavorare con questi strumenti e di anticipare ciò che verrà.

L'eccitante verità è che non lo sappiamo davvero. Sebbene le industrie creative, dai media all'intrattenimento, dalla moda all'architettura, al marketing e altro ancora, sentiranno per prime l'impatto, questa tecnologia darà superpoteri creativi a tutti. A lungo termine, potrebbe essere utilizzata per generare progetti per quasi tutto, da nuovi tipi di farmaci a vestiti ed edifici. La rivoluzione generativa è iniziata.

### Una rivoluzione magica 

Per Chad Nelson, un creatore digitale che ha lavorato a videogiochi e programmi televisivi, i modelli text-to-image sono una scoperta unica nella vita. "Questa tecnologia consente di passare da una lampadina in testa a un primo schizzo in pochi secondi", afferma. "La velocità con cui si può creare ed esplorare è rivoluzionaria, al di là di tutto ciò che ho sperimentato in 30 anni".

A poche settimane dal loro debutto, le persone usavano questi strumenti per prototipare e fare brainstorming su qualsiasi cosa, dalle illustrazioni per le riviste ai layout di marketing, dalle ambientazioni per i videogiochi ai concept per i film. Le persone hanno generato fan art, persino interi fumetti, e li hanno condivisi online a migliaia. Altman ha persino usato DALL-E per generare disegni di scarpe da ginnastica che qualcuno ha poi realizzato per lui dopo aver twittato l'immagine. 

Amy Smith, scienziata informatica della Queen Mary University di Londra e tatuatrice, ha utilizzato DALL-E per progettare tatuaggi. "Puoi sederti con il cliente e generare i disegni insieme", dice. "Siamo in una rivoluzione della generazione dei media".

Paul Trillo, artista digitale e video con sede in California, pensa che la tecnologia renderà più facile e veloce il brainstorming di idee per gli effetti visivi. "La gente dice che questa è la morte degli artisti dell'effettistica, o la morte degli stilisti", afferma Trillo. "Non credo che sia la morte di qualcosa. Penso che significhi che non dobbiamo lavorare di notte e nei fine settimana".

Le aziende produttrici di immagini stock stanno prendendo posizioni diverse. Getty ha vietato le immagini generate dall'intelligenza artificiale. Shutterstock ha firmato un accordo con OpenAI per incorporare DALL-E nel suo sito web e ha dichiarato che avvierà un fondo per rimborsare gli artisti il cui lavoro è stato utilizzato per addestrare i modelli.

Stevenson dice di aver provato DALL-E in tutte le fasi del processo che uno studio di animazione utilizza per produrre un film, compresa la progettazione di personaggi e ambienti. Con DALL-E, è stato in grado di svolgere il lavoro di più reparti in pochi minuti. "È edificante per tutte le persone che non sono mai state in grado di creare perché era troppo costoso o troppo tecnico", dice. "Ma è terrificante se non si è aperti al cambiamento".

Nelson ritiene che ci sia ancora molto da fare. Alla fine, vede questa tecnologia abbracciata non solo dai giganti dei media, ma anche dagli studi di architettura e design. Tuttavia, secondo Nelson, non è ancora pronta.

"In questo momento è come avere una piccola scatola magica, un piccolo mago", dice. È un'ottima cosa se si vuole solo continuare a generare immagini, ma non se si ha bisogno di un partner creativo. "Se voglio che crei storie e costruisca mondi, deve essere molto più consapevole di ciò che sto creando", dice. 

Questo è il problema: questi modelli non hanno ancora idea di cosa stiano facendo.

### All'interno della scatola nera

Per capire perché, analizziamo il funzionamento di questi programmi. Dall'esterno, il software è una scatola nera. Si digita una breve descrizione, un prompt, e si attende qualche secondo. Il risultato è una manciata di immagini che corrispondono alla richiesta (più o meno). Può essere necessario modificare il testo per indurre il modello a produrre qualcosa di più simile a ciò che si aveva in mente, o per affinare un risultato casuale. Questo fenomeno è noto come ingegneria dei prompt.

I prompt per le immagini più dettagliate e stilizzate possono arrivare a diverse centinaia di parole, e trovare le parole giuste è diventata un'abilità preziosa. Sono sorti mercati online dove si comprano e si vendono prompt noti per produrre risultati desiderabili. 

I suggerimenti possono contenere frasi che istruiscono il modello a scegliere un particolare stile: "trending on ArtStation" indica all'intelligenza artificiale di imitare lo stile (in genere molto dettagliato) delle immagini popolari su ArtStation, un sito web in cui migliaia di artisti espongono le loro opere; "Unreal engine" richiama lo stile grafico familiare di alcuni videogiochi; e così via. Gli utenti possono persino inserire i nomi di artisti specifici e far sì che l'IA produca dei pastiche del loro lavoro, cosa che ha reso alcuni artisti molto insoddisfatti.

I modelli text-to-image hanno due componenti chiave: una rete neurale addestrata per abbinare un'immagine a un testo che la descrive e un'altra addestrata per generare immagini da zero. L'idea di base è quella di far sì che la seconda rete neurale generi un'immagine che la prima rete neurale accetti come corrispondente al testo richiesto.

La grande novità dei nuovi modelli sta nel modo in cui vengono generate le immagini. La prima versione di DALL-E utilizzava un'estensione della tecnologia alla base del modello linguistico GPT-3 di OpenAI, che produceva immagini prevedendo il pixel successivo in un'immagine come se fossero parole di una frase. Ha funzionato, ma non bene. "Non è stata un'esperienza magica", dice Altman. "È incredibile che abbia funzionato".

DALL-E 2 utilizza invece un modello di diffusione. I modelli di diffusione sono reti neurali addestrate a ripulire le immagini rimuovendo il rumore dei pixel aggiunto dal processo di addestramento. Il processo prevede che si prendano le immagini e si modifichino pochi pixel alla volta, in più fasi, fino a quando le immagini originali vengono cancellate e rimangono solo pixel casuali. "Se lo si fa migliaia di volte, alla fine l'immagine sembrerà come se avessimo strappato il cavo dell'antenna dal televisore: è solo neve", spiega Björn Ommer, che lavora sull'intelligenza artificiale generativa all'Università di Monaco in Germania e che ha contribuito a costruire il modello di diffusione che ora alimenta Stable Diffusion. 

La rete neurale viene quindi addestrata per invertire il processo e prevedere l'aspetto della versione meno pixelata di una determinata immagine. Il risultato è che se si dà a un modello di diffusione un pasticcio di pixel, esso cercherà di generare qualcosa di più pulito. Se si inserisce nuovamente l'immagine ripulita, il modello produrrà qualcosa di ancora più pulito. Se si esegue questa operazione un numero sufficiente di volte, il modello può portarci dalla neve della TV a un'immagine ad alta risoluzione.

> I generatori di arte AI non funzionano mai esattamente come si desidera. Spesso producono risultati orrendi che, nella migliore delle ipotesi, possono assomigliare a stock art distorte. Secondo la mia esperienza, l'unico modo per rendere il lavoro davvero gradevole è aggiungere un descrittore alla fine con uno stile che risulti esteticamente gradevole.*~Erik Carter*

Il trucco dei modelli da testo a immagine è che questo processo è guidato dal modello linguistico che cerca di abbinare un messaggio alle immagini prodotte dal modello di diffusione. Questo spinge il modello di diffusione verso le immagini che il modello linguistico considera una buona corrispondenza. 

Ma i modelli non estraggono dal nulla i collegamenti tra testo e immagini. La maggior parte dei modelli da testo a immagine oggi viene addestrata su un grande insieme di dati chiamato LAION, che contiene miliardi di abbinamenti di testo e immagini prelevati da Internet. Ciò significa che le immagini ottenute da un modello text-to-image sono un distillato del mondo rappresentato online, distorto dai pregiudizi (e dalla pornografia).

Un'ultima cosa: c'è una piccola ma fondamentale differenza tra i due modelli più diffusi, DALL-E 2 e Diffusione stabile. Il modello di diffusione di DALL-E 2 funziona su immagini a grandezza naturale. Stable Diffusion, invece, utilizza una tecnica chiamata diffusione latente, inventata da Ommer e dai suoi colleghi. Funziona su versioni compresse di immagini codificate all'interno della rete neurale in quello che è noto come spazio latente, in cui vengono mantenute solo le caratteristiche essenziali di un'immagine.

Ciò significa che Stable Diffusion richiede meno potenza di calcolo per funzionare. A differenza di DALL-E 2, che gira sui potenti server di OpenAI, Stable Diffusion può essere eseguito su (buoni) personal computer. Gran parte dell'esplosione di creatività e del rapido sviluppo di nuove applicazioni è dovuta al fatto che Stable Diffusion è sia open source - i programmatori sono liberi di modificarla, costruirci sopra e guadagnarci sopra - sia abbastanza leggera da poter essere eseguita a casa.

### Ridefinire la creatività
Per alcuni, questi modelli sono un passo avanti verso l'intelligenza artificiale generale, o AGI, una parola d'ordine eccessivamente pubblicizzata che si riferisce a un'IA futura con capacità generiche o addirittura simili a quelle umane. OpenAI è stato esplicito sul suo obiettivo di raggiungere l'AGI. Per questo motivo, ad Altman non interessa che DALL-E 2 sia in concorrenza con una serie di strumenti simili, alcuni dei quali gratuiti. "Siamo qui per creare AGI, non generatori di immagini", dice. "Si inserirà in una road map di prodotti più ampia. È un piccolo elemento di ciò che farà un'AGI".

Si tratta di un'affermazione a dir poco ottimistica: molti esperti ritengono che l'IA di oggi non raggiungerà mai questo livello. In termini di intelligenza di base, i modelli testo-immagine non sono più intelligenti delle IA che generano il linguaggio che li sostengono. Strumenti come GPT-3 e PaLM di Google rigurgitano modelli di testo ingeriti dai molti miliardi di documenti su cui vengono addestrati. Allo stesso modo, DALL-E e Stable Diffusion riproducono associazioni tra testo e immagini trovate in miliardi di esempi online.

I risultati sono sorprendenti, ma se si colpisce troppo forte l'illusione va in frantumi. Questi modelli fanno delle semplici bizze: rispondono a "salmone in un fiume" con un'immagine di filetti tagliati a pezzi che galleggiano a valle, o a "una mazza che vola sopra uno stadio di baseball" con un'immagine di un mammifero volante e di un bastone di legno. Questo perché sono costruiti sulla base di una tecnologia che non è neanche lontanamente in grado di comprendere il mondo come fanno gli esseri umani (o la maggior parte degli animali).

Tuttavia, potrebbe essere solo una questione di tempo prima che questi modelli imparino trucchi migliori. "La gente dice che ora non è molto bravo in questa cosa, e ovviamente non lo è", dice Cook. "Ma dopo un centinaio di milioni di dollari potrebbe diventarlo".

Questo è certamente l'approccio di OpenAI.

"Sappiamo già come migliorarlo di 10 volte", dice Altman. "Sappiamo che ci sono attività di ragionamento logico che non funzionano. Faremo un elenco di cose e pubblicheremo una nuova versione che risolva tutti i problemi attuali".

Se le affermazioni sull'intelligenza e la comprensione sono esagerate, che dire della creatività? Tra gli esseri umani, diciamo che artisti, matematici, imprenditori, bambini dell'asilo e i loro insegnanti sono tutti esempi di creatività. Ma è difficile capire cosa abbiano in comune queste persone.

Per alcuni, sono i risultati che contano di più. Altri sostengono che è fondamentale il modo in cui le cose vengono realizzate e se c'è un intento in quel processo.

Tuttavia, molti si rifanno alla definizione data da Margaret Boden, influente ricercatrice nel campo dell'intelligenza artificiale e filosofa dell'Università del Sussex, nel Regno Unito, che riduce il concetto a tre criteri chiave: per essere creativi, un'idea o un artefatto devono essere nuovi, sorprendenti e di valore.

Al di là di questo, spesso si tratta di riconoscerlo quando lo si vede. I ricercatori del campo noto come creatività computazionale descrivono il loro lavoro come l'utilizzo di computer per produrre risultati che sarebbero considerati creativi se prodotti da soli esseri umani. 

Smith è quindi felice di definire creativi questi nuovi modelli generativi, nonostante la loro stupidità. "È molto chiaro che in queste immagini c'è un'innovazione che non è controllata da alcun input umano", afferma la Smith. "La traduzione dal testo all'immagine è spesso sorprendente e bella".

Maria Teresa Llano, che studia la creatività computazionale alla Monash University di Melbourne, in Australia, concorda sul fatto che i modelli da testo a immagine stiano estendendo le definizioni precedenti. Ma Llano non pensa che siano creativi. Quando si usano spesso questi programmi, i risultati possono iniziare a diventare ripetitivi, dice. Ciò significa che non soddisfano alcuni o tutti i requisiti di Boden. E questo potrebbe essere un limite fondamentale della tecnologia. Per sua natura, un modello da testo a immagine sforna nuove immagini a somiglianza di miliardi di immagini già esistenti. Forse l'apprendimento automatico produrrà solo immagini che imitano quelle a cui è stato esposto in passato.

Questo potrebbe non essere importante per la computer grafica. Adobe sta già inserendo la generazione di testo-immagine in Photoshop; Blender, il cugino open-source di Photoshop, ha un plug-in di Diffusione stabile. E OpenAI sta collaborando con Microsoft per la creazione di un widget text-to-image per Office.

È in questo tipo di interazione, nelle versioni future di questi strumenti familiari, che si potrà sentire il vero impatto: da macchine che non sostituiscono la creatività umana, ma la potenziano. La creatività che vediamo oggi proviene dall'uso dei sistemi, piuttosto che dai sistemi stessi", dice Llano, "dalle chiamate e dalle risposte necessarie per produrre il risultato desiderato".

Questo punto di vista è condiviso da altri ricercatori sulla creatività computazionale. Non si tratta solo di quello che fanno queste macchine, ma anche di come lo fanno. Trasformarle in veri e propri partner creativi significa spingerle a essere più autonome, dare loro responsabilità creative, far sì che curino e creino.

Aspetti di questo tipo arriveranno presto. Qualcuno ha già scritto un programma chiamato CLIP Interrogator che analizza un'immagine e propone un prompt per generare altre immagini simili. Altri stanno usando l'apprendimento automatico per aumentare i prompt semplici con frasi progettate per dare all'immagine una qualità e una fedeltà extra, automatizzando di fatto l'ingegneria dei prompt, un'attività che esiste solo da pochi mesi.

Nel frattempo, mentre la marea di immagini continua, stiamo gettando anche altre basi. "Internet è ormai perennemente contaminato da immagini realizzate dall'intelligenza artificiale", afferma Cook. "Le immagini che abbiamo realizzato nel 2022 faranno parte di qualsiasi modello realizzato d'ora in poi". 

Dovremo aspettare per vedere esattamente quale impatto duraturo avranno questi strumenti sulle industrie creative e sull'intero campo dell'IA. L'IA generativa è diventata un ulteriore strumento di espressione. Altman dice che ora usa le immagini generate nei messaggi personali come prima usava le emoji. "Alcuni dei miei amici non si preoccupano nemmeno di generare l'immagine: digitano il messaggio", dice.

Ma i modelli da testo a immagine potrebbero essere solo l'inizio. L'intelligenza artificiale generativa potrebbe essere utilizzata per produrre progetti per qualsiasi cosa, da nuovi edifici a nuovi farmaci - si pensi al text-to-X. 

"Le persone si renderanno conto che la tecnica o il mestiere non sono più una barriera, ma solo la loro capacità di immaginare", afferma Nelson.

I computer sono già utilizzati in diversi settori per generare un gran numero di progetti possibili che vengono poi vagliati per trovare quelli che potrebbero funzionare. I modelli Text-to-X consentirebbero a un progettista umano di perfezionare il processo generativo fin dall'inizio, usando le parole per guidare i computer attraverso un numero infinito di opzioni verso risultati non solo possibili, ma desiderabili.

I computer possono creare spazi pieni di infinite possibilità. Text-to-X ci permetterà di esplorare quegli spazi usando le parole.

"Penso che questa sia l'eredità", dice Altman. "Immagini, video, audio, alla fine tutto sarà generato. Penso che si diffonderà ovunque".

### Gli artisti possono ora scegliere di non partecipare alla prossima versione di Stable Diffusion

Cosa è successo: Gli artisti possono ora scegliere di non partecipare alla prossima versione di uno dei generatori di IA da testo a immagine più popolari al mondo, Stable Diffusion, ha annunciato la società che lo ha creato. I creatori possono cercare su un sito web chiamato HaveIBeenTrained le loro opere nell'insieme di dati utilizzati per l'addestramento di Stable Diffusion e selezionare le opere che vogliono escludere dai dati di addestramento.

Perché è importante: la decisione arriva nel mezzo di un acceso dibattito pubblico tra artisti e aziende tecnologiche sulle modalità di addestramento dei modelli di intelligenza artificiale da testo a immagine. La coppia di artisti che ha creato il sito web spera che il servizio di opt-out compensi temporaneamente l'assenza di una legislazione che regoli il settore.

—Melissa Heikkilä

[[Notes]]